{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classify images as cat or dog:\n",
    "    1) Train convnet from scratch without data augmentation\n",
    "    2) Train convnet from scratch with data augmentation (use of image generators)\n",
    "    3) Use a pre-trained convnet for feature extraction and train a classifier on top of it with data augmentation\n",
    "    4) Fine-tune a pre-trained convnet for feature extration with a pre-trained classifier on top of it (use of image generators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### Define the IO directories of the cat/dog dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "original_dataset_dir = \"catOrDog_dataset\"\n",
    "original_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "\n",
    "base_dir = \"catOrDog_dataset_organized\"\n",
    "\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "validation_dir = os.path.join(base_dir, \"validation\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, \"cats\")\n",
    "train_dogs_dir = os.path.join(train_dir, \"dogs\")\n",
    "\n",
    "validation_cats_dir = os.path.join(validation_dir, \"cats\")\n",
    "validation_dogs_dir = os.path.join(validation_dir, \"dogs\")\n",
    "\n",
    "test_cats_dir = os.path.join(test_dir, \"cats\")\n",
    "test_dogs_dir = os.path.join(test_dir, \"dogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### Create base_dir with the respective subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "os.mkdir(base_dir)\n",
    "os.mkdir(train_dir)\n",
    "os.mkdir(validation_dir)\n",
    "os.mkdir(test_dir)\n",
    "os.mkdir(train_cats_dir)\n",
    "os.mkdir(train_dogs_dir)\n",
    "os.mkdir(validation_cats_dir)\n",
    "os.mkdir(validation_dogs_dir)\n",
    "os.mkdir(test_cats_dir)\n",
    "os.mkdir(test_dogs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### Copy files from original_dataset_dir to base_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fnames = [\"cat.{}.jpg\".format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "fnames = [\"cat.{}.jpg\".format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "fnames = [\"cat.{}.jpg\".format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "fnames = [\"dog.{}.jpg\".format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "fnames = [\"dog.{}.jpg\".format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "fnames = [\"dog.{}.jpg\".format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2) Train convnet from scratch with data augmentation (use of image generators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### Build up the convnet.\n",
    "    Other than the first and last layers, you have the liberty to control how the network is going to be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, \n",
    "                        (3,3), \n",
    "                        activation=\"relu\", \n",
    "                        input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64, (3,3), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3,3), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128, (3,3), activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###### Summarize the model. \n",
    "    Show the total number of hyperparameters to be optimized by the end of the list. Note how max pooling downgrades the input of the previous layer from (n x n) to (n/2 x n/2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### Compile the model with:\n",
    "    1) loss = binary cross entropy. Because it is a binary problem and the data is well-balanced. \n",
    "    2) optimizer = RMSprop with learning rate = 0.00004\n",
    "    3) evaluation metrics = accuracy over validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "###### REMINDER: once the model is compiled, you need to recompile it whenever you want to change some of its parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### Instantiate image generators. \n",
    "    In this setting, you feed the model with a stream of data points coming from the dataset. It is a common practice to play with data generators if you want to employ data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True, fill_mode=\"nearest\")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "###### Data augmentation generates images from a given data by performing a random operation that does still characterizes the generated data as coming from the original data. Examples:\n",
    "    1) cat img -> rotate -> generated img\n",
    "    2) cat img -> translate + rotate -> generated img\n",
    "    3) cat img -> crop -> generated img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### Instantiate objects from data generators. \n",
    "    The batch_size option tells us how many images are going to be fed into the model each time. It is important to note that each random image operation is performed in parallel by the GPU (if you have enabled that option). Thus is it wise to give a number that is \"easy\" to parallelize with your GPU. \n",
    "\n",
    "###### NOTE: flow_from_directory method automatically understands that each subdirectory in base_dir (\\cat or \\dog) is with respect with a different class so it assigns each class with a label. In this case, class_mode=\"binary\" so Y = {0, 1}.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150,150),\n",
    "    batch_size=32,\n",
    "    class_mode=\"binary\"\n",
    ")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150,150),\n",
    "    batch_size=20,\n",
    "    class_mode=\"binary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###### Train the compiled model using train_generator. Here is a short explanation of what each parameter means:\n",
    "    1) steps_per_epoch = every time train_generator feeds the model, it is one step. So steps_per_epoch * batch_size is going to be the number of data points your model is going to consider one epoch. \n",
    "    2) epochs = the number of epochs based on the definition of 1 epoch = steps_per_epoch * batch_size. \n",
    "    3) validation_steps = similar to steps_per_epoch. Validation_steps * batch_size = 1 validation dataset epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 35s 349ms/step - loss: 0.6890 - acc: 0.5294 - val_loss: 0.6868 - val_acc: 0.5050\n",
      "Epoch 2/100\n",
      " 51/100 [==============>...............] - ETA: 11s - loss: 0.6828 - acc: 0.5643"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-760af726713a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0;31m# Make sure to rethrow the first exception in the queue, if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=100,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "    Save the model as an h5 file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e1fbdaa65baf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cats_and_dogs_small_4.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.save(\"cats_and_dogs_small_4.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "    Output plots showing the model performance as it was trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history[\"acc\"]\n",
    "val_acc = history.history[\"val_acc\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_los = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, \"bo\", label=\"training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"validation acc\")\n",
    "plt.title(\"training and validation acc\")\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Use a pre-trained convnet for feature extraction and train a classifier on top of it with data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Instantiate pretrained (weights=\"imagenet\") convnet (VGG16) without the densely connected layer on top (include_top=False)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "conv_base = VGG16(weights=\"imagenet\",\n",
    "                 include_top=False,\n",
    "                 input_shape=(150, 150, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 150, 150, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The final feature has shape (4, 4, 512). That's the featre on top of which you will stick a densely connected classifier. Or any other classifier. You might wanna test it with a SVM or logistic regression (use sckit-learn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, 4, 4, 512))\n",
    "    labels = np.zeros(shape=(sample_count))\n",
    "    generator = datagen.flow_from_directory(directory,\n",
    "                                            target_size=(150,150), \n",
    "                                            batch_size=batch_size, \n",
    "                                            class_mode=\"binary\")\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch) # this is the fit function\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count: \n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = extract_features(train_dir, 2000)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 1000)\n",
    "test_features, test_labels = extract_features(test_dir, 1000)\n",
    "\n",
    "train_features = np.reshape(train_features, (2000, 4 * 4 * 512))\n",
    "validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))\n",
    "test_features = np.reshape(test_features, (1000, 4 * 4 * 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 3s 2ms/step - loss: 0.5952 - acc: 0.6760 - val_loss: 0.4401 - val_acc: 0.8290\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.4337 - acc: 0.8015 - val_loss: 0.3585 - val_acc: 0.8620\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.3617 - acc: 0.8455 - val_loss: 0.3267 - val_acc: 0.8720\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.3170 - acc: 0.8625 - val_loss: 0.2991 - val_acc: 0.8850\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.2900 - acc: 0.8815 - val_loss: 0.2808 - val_acc: 0.8930\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.2639 - acc: 0.8955 - val_loss: 0.2717 - val_acc: 0.8960\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.2475 - acc: 0.9035 - val_loss: 0.2739 - val_acc: 0.8790\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.2328 - acc: 0.9080 - val_loss: 0.2590 - val_acc: 0.8950\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.2194 - acc: 0.9150 - val_loss: 0.2535 - val_acc: 0.8930\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.2166 - acc: 0.9180 - val_loss: 0.2635 - val_acc: 0.8800\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1963 - acc: 0.9285 - val_loss: 0.2457 - val_acc: 0.9050\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1869 - acc: 0.9290 - val_loss: 0.2508 - val_acc: 0.8910\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1791 - acc: 0.9355 - val_loss: 0.2407 - val_acc: 0.9070\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1662 - acc: 0.9450 - val_loss: 0.2421 - val_acc: 0.8980\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1632 - acc: 0.9425 - val_loss: 0.2373 - val_acc: 0.9080\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1527 - acc: 0.9470 - val_loss: 0.2363 - val_acc: 0.9060\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1444 - acc: 0.9530 - val_loss: 0.2340 - val_acc: 0.9110\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1462 - acc: 0.9515 - val_loss: 0.2387 - val_acc: 0.8970\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1327 - acc: 0.9580 - val_loss: 0.2332 - val_acc: 0.9100\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1297 - acc: 0.9625 - val_loss: 0.2454 - val_acc: 0.8980\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1231 - acc: 0.9565 - val_loss: 0.2348 - val_acc: 0.9050\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1250 - acc: 0.9600 - val_loss: 0.2458 - val_acc: 0.8980\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1164 - acc: 0.9640 - val_loss: 0.2330 - val_acc: 0.9120\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1088 - acc: 0.9635 - val_loss: 0.2335 - val_acc: 0.9100\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1030 - acc: 0.9695 - val_loss: 0.2345 - val_acc: 0.9080\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.1007 - acc: 0.9670 - val_loss: 0.2337 - val_acc: 0.9070\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.0972 - acc: 0.9700 - val_loss: 0.2389 - val_acc: 0.9010\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.0960 - acc: 0.9695 - val_loss: 0.2438 - val_acc: 0.9030\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.0914 - acc: 0.9730 - val_loss: 0.2351 - val_acc: 0.9050\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 2s 1ms/step - loss: 0.0883 - acc: 0.9715 - val_loss: 0.2409 - val_acc: 0.9040\n"
     ]
    }
   ],
   "source": [
    "from keras import models \n",
    "from keras import layers \n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation=\"relu\", input_dim=4*4*512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "             loss=\"binary_crossentropy\",\n",
    "             metrics=[\"acc\"])\n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                   epochs=30,\n",
    "                   batch_size=20,\n",
    "                   validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de3wU1f3/8deHCAKKioKVi0nQ2q/cgsEUpVbEeqP2Wy9UK0jrvXz1p9RWrVXxgvaLWqvW1ju2tv1qLCJqxVa8IFC13ogKWFGUikgAIXKTOwQ+vz/OBDbJJtlNsmwyeT8fj33szsyZmTM7yWfOnjnnjLk7IiISX62ynQEREcksBXoRkZhToBcRiTkFehGRmFOgFxGJOQV6EZGYU6CXtJnZA2Z2XWOnzSYzm25mF2Rgu5+Z2bHR52vM7A+ppK3Hfo40s7n1zafE2y7ZzoDsXGb2GXCBu0+p7zbc/cJMpI07d7+5sbZlZg4c5O7zom2/CvxXY21f4kUleqnEzHTxF4kZBfoWxMweAXKBZ81srZldaWb5ZuZmdr6ZfQ5MjdI+YWZfmNlqM3vFzHonbOfPZva/0efBZlZqZpeb2TIzW2Jm59Yz7T5m9qyZfWVmM8zsf83stVqOp6483mtm/zCzNWb2lpkdmLD8ODP7KFr3HsBq2EdXM9tgZnsnzCs0sy/NrLWZHWhmU81seTSv2Mz2qmFbY8zs0YTpH5vZgmjd0VXSDjCzN8xsVfQ93WNmbaJlr0TJZkXn8YyK7zZh/Z5RddQqM/vAzE5K9btJ83tuZ2Z3RMex2sxeM7N20bJvm9nrUR4Wmtk5Ne1DMkuBvgVx9x8DnwPfd/fd3f22hMVHAT2BE6LpycBBwL7Au0BxLZveD9gT6AacD9xrZh3rkfZeYF2U5uzoVZu68jgcuBHoCMwDxgKYWSfgSeBaoBPwH+CIZDtw98XAG8APEmafCUx09y2EC8QtQFfC97c/MKaOfGNmvYD7gR9H6+4DdE9IshX4eZS/gcAxwP+L8jQoStMvOo+PV9l2a+BZ4EXCdzMKKDazxKqdpN9NDWr7nm8HDgW+BewNXAlsM7PcaL27gc7AIcDM2r4TySB316sFvYDPgGMTpvMBBw6oZZ29ojR7RtN/Bv43+jwY2ADskpB+GXB4OmmBHGAL8F8Jy/4XeC3F40qWxz8kLD8R+Cj6fBbwZsIyA0oJ9y6SbfsCYGpC2oXAoBrSngK8l+z7JlwAHo0+Xw+MT0i3G7A58dxU2e7PgKcTph34esL0YKA0+nwk8AXQKmH5X4ExdX036XzPhILiBsIFp2q6qxPzq1d2XyrRS4WFFR/MLMfMbjWz/5jZV4RgBaF0mcxydy9PmF4P7J5m2s6ExgELE5Ylfq4kxTx+UUOeuiZu20NkqnFfwERgoJl1BQYRAt2rUT72NbPxZrYoysej1Pw9Jaqah3XA8oTj+4aZ/T2qMvkKuDnF7W7ftrtvS5i3gPArqkJN300ldXzPnYC2hF9EVe1fw3zJAgX6lqem4UoT558JnAwcSyi55Ufzk9ZjN5IyoJzK1Rf715K+IXlckrhtM7Pa9uXuqwjVID+M9vvX6OIAodrGgQJ33wP4UT3z0J5QfVPhfuAjQsuaPYBrUtwuwGJgfzNL/P/OBRaluH6i2r7nL4GNQLL6/YU1zJcsUKBveZYCB9SRpgOwiVDCbE8oTWaUu28FngLGmFl7MzuYUMWSiTz+A+htZkMttDL6KeG+QG0ei/Lzg+hzYj7WAqvMrBvwixTzMBH47+iGZRvgJir/P3YAvgLWRt/FRVXWr+08vkW413FldMN4MPB9YHyKeUtU4/cc/WJ4GLgzummdY2YDzWxXQj3+sWb2QzPbJbrRfkg99i+NQIG+5bkFuDZqCXFFDWn+j/BTfxEwB3hzJ+XtEkKp8QvgEUK98qYa0tY7j+7+JXA6cCshgB0E/KuO1SZF6Za6+6yE+TcC/YHVhAvIUynm4QPgYsJFYwmwknCfoMIVhNL0GuAh4PEqmxgD/CU6jz+ssu3NwEnAdwml7vuAs9z9o1TyVkVd3/MVwPvADGAF8GvCvYHPCXX/l0fzZwL96rF/aQS24xeoSNNiZr8G9nP3ulrfiEgtVKKXJsPMDjazAgsGEJpfPp3tfIk0d+oFKU1JB0J1TVdCs8s7gGeymiORGFDVjYhIzKnqRkQk5ppc1U2nTp08Pz8/29kQEWlW3nnnnS/dvXOyZU0u0Ofn51NSUpLtbIiINCtmtqCmZaq6ERGJOQV6EZGYU6AXEYm5JldHn8yWLVsoLS1l48aN2c6K1KBt27Z0796d1q1bZzsrIlJFswj0paWldOjQgfz8fMJAg9KUuDvLly+ntLSUHj16ZDs7IlJFs6i62bhxI/vss4+CfBNlZuyzzz76xSVSRXEx5OdDq1bhvbi257RlULMI9ICCfBOn8yNSWXExjBwJCxaAe3gfOTJ5sM/0BaHZBHoRkUxq7GA7ejSsX1953vr1YX7V/aZ6QagvBfoUrFq1ivvuu69e65544omsWrWq1jTXX389U6ZMqdf2RVqixg7KmSh9f/55avNTvSA0SLYfWlv1deihh3pVc+bMqTavNo8+6p6X524W3h99NK3Vq5k/f7737t076bLy8vKGbTxG0j1PIvXx6KPu7du7h5AcXu3bJ/8/TzUW5OVV3l7FKy+v/vtOdZtmydOZpfe9ACVeQ1zNemCv+mpooE/nRKTqjDPO8LZt23q/fv38iiuu8GnTpvngwYN9+PDh3rNnT3d3P/nkk71///7eq1cvf/DBB7evm5eX52VlZT5//nw/+OCD/YILLvBevXr5cccd5+vXr3d397PPPtufeOKJ7emvv/56Lyws9D59+viHH37o7u7Lli3zY4891gsLC33kyJGem5vrZWVl1fJ64YUX+qGHHuq9evXy66+/fvv8t99+2wcOHOgFBQX+zW9+07/66isvLy/3yy+/3Pv06eN9+/b13//+9/X/klyBXnaOTATlVINtqvtOZ//pbLM2LSrQN9aXlqhqiX7atGnevn17//TTT7fPW758ubu7r1+/3nv37u1ffvlllJ8dgT4nJ8ffe+89d3c//fTT/ZFHHnH36oG+IuDee++9fv7557u7+8UXX+w333yzu7tPnjzZgaSBviIf5eXlftRRR/msWbN806ZN3qNHD3/77bfd3X316tW+ZcsWv++++3zo0KG+ZcuWSuvWlwK9JNPYv7AzEZQzVfpO5dgbq3BaW6CPXR19qvViDTVgwIBKbcZ///vf069fPw4//HAWLlzIJ598Um2dHj16cMgh4fnIhx56KJ999lnSbQ8dOrRamtdee41hw4YBMGTIEDp27Jh03QkTJtC/f38KCwv54IMPmDNnDnPnzqVLly5885vfBGCPPfZgl112YcqUKVx44YXsskvoTrH33nun/0VIk5fNJn6ZqPvOzU1tfjqxYOxYaN++8rz27cP8+uy7wogR8NlnsG1beB8xInmaceMgLw/Mwvu4ccnT1lfsAn26J6K+dtttt+2fp0+fzpQpU3jjjTeYNWsWhYWFSduU77rrrts/5+TkUF5ennTbFekS04QLdu3mz5/P7bffzssvv8zs2bP53ve+x8aNG3H3pM0fa5ov8bEzWnTUJhMtTzIRlFMNtqnuO12pXBAaInaBPhMnokOHDqxZs6bG5atXr6Zjx460b9+ejz76iDfffLP+O6vBt7/9bSZMmADAiy++yMqVK6ul+eqrr9htt93Yc889Wbp0KZMnTwbg4IMPZvHixcyYMQOANWvWUF5ezvHHH88DDzyw/WKyYsWKRs+3ZFemWnRks+VJpoJyUyl9Z0LsAn0mTsQ+++zDEUccQZ8+ffjFL35RbfmQIUMoLy+noKCA6667jsMPP7wBR5DcDTfcwIsvvkj//v2ZPHkyXbp0oUOHDpXS9OvXj8LCQnr37s15553HEUccAUCbNm14/PHHGTVqFP369eO4445j48aNXHDBBeTm5lJQUEC/fv147LHHGj3fkjmpBNtMVGWmU/rORDULZDcoZ7r0nRE1Vd5n69UYzSvjaOPGjdtvmr7++uver1+/LOeoOp2nnSdTLTpSuXnYlFuetGS0pFY3cfXxxx/7IYcc4gUFBV5UVLS9BU1TovO082SiiWGqaZtyy5OWTIFedgqdp50nnWDb2J2GMlX6buxmmC1NbYE+dnX0Ik1RYzdxTLdFSSp1yqnWkzfXlictWUqB3syGmNlcM5tnZlclWZ5nZi+b2Wwzm25m3ROWbTWzmdFrUmNmXqQ5yERb8kwE21QvHs215UmLVlNRv+IF5AD/AQ4A2gCzgF5V0jwBnB19/g7wSMKytXXtI/GlqpvmS+cpuUzUp1ekb8yqDtWTN280sOpmADDP3T91983AeODkKml6AS9Hn6clWS7SYmVqFMPGrupQST2+Ugn03YCFCdOl0bxEs4AfRJ9PBTqY2T7RdFszKzGzN83slAblthnZfffdAVi8eDGnnXZa0jSDBw+mpKSk1u3cddddrE/4709l2GPZObLZZT9TVE8eT6kE+mR95Kv2x78COMrM3gOOAhYBFf37c929CDgTuMvMDqy2A7OR0cWgpKysLPXcNwNdu3Zl4sSJ9V6/aqB/7rnn2GuvvRojay1KNscvz9Q4KiKpSiXQlwL7J0x3BxYnJnD3xe4+1N0LgdHRvNUVy6L3T4HpQGHVHbj7OHcvcveizp071+c4MuqXv/xlpQePjBkzhjvuuIO1a9dyzDHH0L9/f/r27cszzzxTbd3PPvuMPn36ALBhwwaGDRtGQUEBZ5xxBhs2bNie7qKLLqKoqIjevXtzww03AGGgtMWLF3P00Udz9NFHA5Cfn8+XX34JwJ133kmfPn3o06cPd9111/b99ezZk5/85Cf07t2b448/vtJ+Kjz77LMcdthhFBYWcuyxx7J06VIA1q5dy7nnnkvfvn0pKCjgySefBOD555+nf//+9OvXj2OOOabB3+nOlInxXppCl32RlNVUeV/xAnYBPgV6sONmbO8qaToBraLPY4Gbos8dgV0T0nxClRu5VV913Yy99FL3o45q3Nell9Z+k+Pdd9/1QYMGbZ/u2bOnL1iwwLds2eKrV692d/eysjI/8MADfdu2be7uvttuu7l75SGO77jjDj/33HPd3X3WrFmek5PjM2bMcPfkwwu77xjmuELFdElJiffp08fXrl3ra9as8V69evm7775b63DIiVasWLE9rw899JBfdtll7u5+5ZVX+qUJX8iKFSt82bJl3r179+3DMtc0nHFTvRmbid6hjfWwiPrsWyQZarkZu0sKF4JyM7sEeIHQAudhd//AzG6KNjwJGAzcYmYOvAJcHK3eE3jQzLYRfj3c6u5zGnZp2vkKCwtZtmwZixcvpqysjI4dO5Kbm8uWLVu45ppreOWVV2jVqhWLFi1i6dKl7Lfffkm388orr/DTn/4UgIKCAgoKCrYvmzBhAuPGjaO8vJwlS5YwZ86cSsureu211zj11FO3j6I5dOhQXn31VU466aSUhkMuLS3ljDPOYMmSJWzevHn7kMtTpkxh/Pjx29N17NiRZ599lkGDBm1P09yGM06n7rui9F9RWq8o/UPlEnhublhWVUOrWUaMUL24NL46Az2Auz8HPFdl3vUJnycC1Sqi3f11oG8D81hJVEOx05122mlMnDiRL774Yvu48MXFxZSVlfHOO+/QunVr8vPzkw5PnCjZsMAVwwvPmDGDjh07cs4559S5nXABT67qcMjJqm5GjRrFZZddxkknncT06dMZM2bM9u1WzWOyec1JOkG5tiqZxAA8dmzlCwKomkWaLvWMTdGwYcMYP348EydO3N6KZvXq1ey77760bt2aadOmsSBZNEkwaNAgiqOK4X//+9/Mnj0bqHl4Yah5iORBgwbxt7/9jfXr17Nu3TqefvppjjzyyJSPZ/Xq1XTrFhpP/eUvf9k+//jjj+eee+7ZPr1y5UoGDhzIP//5T+bPnw80v+GM06n7TrX0r6aI0pwo0Keod+/erFmzhm7dutGlSxcARowYQUlJCUVFRRQXF3PwwQfXuo2LLrqItWvXUlBQwG233caAAQOAmocXBhg5ciTf/e53t9+MrdC/f3/OOeccBgwYwGGHHcYFF1xAYWG1+9w1GjNmDKeffjpHHnkknTp12j7/2muvZeXKlfTp04d+/foxbdo0OnfuzLhx4xg6dCj9+vXjjDPOSHk/mZZKa5p0gnImhhYQyTarrQogG4qKirxq2/IPP/yQnj17ZilHkqqdfZ6q1qdDKKk3pGSdiW2K7Axm9o6HpuzVqEQvzVYmnp6kKhmJo5Ruxoo0RZnqSaqWLxI3zaZE39SqmKSybJwf9SQVSU2zCPRt27Zl+fLlCvZNlLuzfPly2rZtW2u6VIchyOZQvSJx1Cyqbrp3705paSlxGwcnTtq2bUv37t1rXJ5qR6RU0yVOjx4dqmtyc0OQV7WLSGXNotWNNH/5+ck7LeXlhaaJ6aYTkcrU6kayLtUbp01hqF6RuFGgl50i1RunusEq0vgU6GWnSPXGqW6wijQ+BXrZKVLtiKQOSyKNTzdjpUGKi9XqRaQpqO1mbLNoXilNUzpNIUUke1R1I0ml0mkpE2PNiEjjU4leqkm1pK6mkCLNg0r0Uk2qJXU1hRRpHhTopZpUS+pqCinSPKjqRqpJ9RmrGmsmdeXlMH8+HHhguO/RkkyfDpdcAm3bhvs9PXpUfs/Lg+gZ95IhCvQtSKpNIdN58LXGbq/Zxo3w0kvw1FPw7LOwfDmcemo4D+3aNXz7K1fCvffC8OHhAtIUPf44nHVW+Hvbf3/497/h73+HTZsqp9t33+QXgR49wrp1DIwqdVCgbyE0KmTq3ENAatcuHHubNqmvu3o1/OMf8PTTMHkyrFsHe+4J//3f0L073HYbHHssTJoE++xT/zzOnQsnnQQffwy33w5/+lO4iDQld94Jl18ORx4Jf/sb7L13mL9tGyxdGgapmz+/8ntJSbgwbtlSeVtdu1a/AOTlQadOsNde4bXHHi3v11Kq1GGqhYjrqJDr14eAcuKJ0L9/w7dXWgoXXxwCMYTeud26VS9pVnzu3h2+/BKeeSYE96lTQ5Dabz84+WQYOhQGD95xsZg4EX70o7D+88+H93S98AKccQa0bg133w133BEC5GWXwa23hvnZtG0bXHEF/Pa3cNpp8Mgj6ZXIt26FxYuTXwjmz4eFC8M+qjILF9WKwF/x6tgRevYM33uXLo10kE1QbR2mFOhjIJUqmVatQkm1KrPk/zSZMn9+yGNOTsO3tWEDfP/78PLL4TjOOScce33+mbduhfvvh2uuCfXpo0eHAF810JSWVv4ec3LC9+ceqk9OPTW8Dj+85tLlq6+G0njbtvDcc1BYmFoe3eF3vwul5N69w8UoPz9Ug1x+eajG+da3QnVJLY8GqNW2bfDhh/CNb9TvgrFxI5x9NkyYAD/9abgIN8a5TrRlSzgPCxaE6qtVq3a8V7wSp1euhEWLQj6GDIHzzgu/sNL5pVYf5eUhn4sWharPiotOpn551Bbocfcm9Tr00ENdUvfoo+7t27uHMBBe7duH+Yny8iqnqXjl5e28vL74Ytjn977nvnZtw7a1YYP7cce5m7nfd5/7FVe4t27tvttu7mPHhuWpmj3b/fDDQ96OO8593rya027aFJZPmeL+0EPuo0e7/+pXYRvbtqW+zw8+cN9/f/fddw/fS102bnQ/77yQx1NOcV+zpnqa8ePD9jp1cn/hhdTz4u6+bp37Aw+4H3xw2Ee3bu633uq+YkXq21ixwn3QoLD+b36T3veRaR995H7VVe5du4b8derk/rOfuc+aVf9tlpe7L1zo/sor7v/3f+433uh+7rnugwe75+e75+Qk/58zc99zz/C/169fSH/KKe7nnON+2231zw9Q4jXE1ZSCLzAEmAvMA65KsjwPeBmYDUwHuicsOxv4JHqdXde+FOjTk2oAT/WCkCnr1rkfcID7177m3qqV+ze/6b50af22tXGj+5Ah4RgefnjH/E8+Cf8wFcc/YULtwWbDhhCod9kl/OM/8sjODU6LFrkXFIT9/+UvNadbutT9iCPCcV17rfvWrTWn/egj9z59QjC57roQjGqzZEnY5j77hO337+/+29+6H3PMjr+Riy92//jj2rfz+efuvXuHi+1jj9WeNpu2bHH/xz/cTzst5BXcDz3U/Z573Jcvr5x227bw/bz+ejimsWPdf/IT92OPdf/613esn/jq2tX9W99yHzEi/G394Q/ukye7P/20+5/+5H7nne7XX+/+05+6n3WW+/e/737kke59+7p37+5+/PH1P7YGBXogB/gPcADQBpgF9KqS5omKIA58B3gk+rw38Gn03jH63LG2/SnQp8es5lJDVY8+GgKgWXjfWUHePZSmwH3aNPdJk9zbtXM/8MAQnNOxcWP4RQChRJ3Myy+HAAru3/62e0lJ9TRTp7ofdFBIc9ZZ7mVlaR9So1i1akdQHTu2+oVm5kz33Fz3tm1DiT0V69aF0iGEbSe7oM6c6X722SFYmbmffLL7P/9Zef8zZ4btJKaZPr16HmfPDr8A9tgjfPfNRVmZ++9+F0rV4N6mjftJJ4VCxMEHh++86v/Vvvu6DxjgfsYZ7r/8ZfgV9Pzz7nPnpvcrMhMaGugHAi8kTF8NXF0lzQcVpXjAgK+iz8OBBxPSPQgMr21/CvTpaQpVMnWZNSv8jD333B3z3nwzlKI7dXJ/663UtrNpU/hHhPAPVpvycvdx49w7dw5B6pxzQgl6+fKQDwi/MF56qf7H1Vg2bQolQHC/6KIdpfAnnwwl6m7dkl+s6vLHP4Zg1aVLqF7YutX97393/853dpTWL7mk7ovt4sXVS/2PPhryPXVqCPDdujWsGiTb3n3XfdSoUOXSv7/7D37gfvnloaT/97+HqraGVjdmWkMD/WnAHxKmfwzcUyXNY8Cl0eehgAP7AFcA1yakuw64Isk+RgIlQElubu7O+l6atFRL39mukqlLebn7YYeFgPvll5WXffxxCLbt2rk/+2zt29m82f3UU8Px3XNP6vtftcr9yitDaW233UI+cnLCL4x169I/nkzZujWUECGUnG+4IXw+7LAQaOtr5szwyyUnJ3zXFfXvv/51evXv7u7r17s/+OCOevyuXcP32rt3qLqR7GpooD89SaC/u0qarsBTwHvA74BSYE/gF0kC/eW17U8l+hCk27VLPXhns0qmLnffHfJfU56++MK9qCjU2z/4YPI0mzeHOlUIP7XrY968sI1Bg0Lwa6ruvntHddyPf9w41QGrV4dfMYMGhbrmzZsbtr2tW92fey7UJ594YvoXDMmM2gJ9nc0rzWwgMMbdT4imrwZw91tqSL878JG7dzez4cBgd/+faNmDwHR3/2tN+2uJzSs3bgwddGbOhPfegz/+sXrPQWicNu+vvBJ6aebmVm4XvvvuDdtuMqWl0KsXDBwY2oybJU+3di388Iehg9F118GNN+5IW14e2j8//nhoqvfznzd+PpuaF14ITfLOPbfm70ykqoY+eGQGcJCZ9QAWAcOAM6vsoBOwwt23EerwH44WvQDcbGYdo+njo+Utlju8+Sa89VYI6u+9F9otl5eH5R06JA/y0PDhfydPhlNOCe2Qq17fO3Wq3iHo61+Ho4+ufwecUaPCcd1/f+0Ba/fdQ4ejCy+EX/0qXCAefDC0NT777BDkb7utZQR5gBNOyHYOJG7qDPTuXm5mlxCCdg7wsLt/YGY3EX4qTAIGA7eYmQOvABdH664ws18RLhYAN7n7igwcR5PnHjr2XH89vPFGmLfffqGzzPe/H94LC0OgPeCA5L1YG9Kr74UXQkeePn3C+CsVg2xV7XU4a1YIups3h/WOOir05uzUKb39Pf106PZ+663heOrSujX84Q9hPJQbb4QlS8I+H3sMbr4ZfvGLdI9YRLarqU4nW6841tFPmxbaykJoK3vffaF9bk2S3WCtaNpVnxtzL70UWl8cckj1tsLJbN0aWqg89JD7rru69+jh/v77qe9v9epww6+goH71wQ89tKOzya9+lf76Ii0RDe0wtTNfcQr0r7zifvTR4Vvu0iW0Ftm4MbV1q95gvfHG0GqkT5/0bn5NnRpu7PbtW7+24m++6b7ffqHH5TPPpLbOJZeEfKfabDKZqVPd//zn+q8v0tIo0O9kr78eutJD6Al6112haVpDTZkSmrMNHJham97p08Mvg9693Zctq/9+Fy4MvQfN3G++ufbeo2+8EdKNGlX//YlI+hTod5K33trRNb9zZ/fbb2/8ttpPPhmaIp5wQuiwUpNXXw2/AHr2DE0YG2r9evdhw8KxnXlm8gvX5s3hl0O3bqH6RkR2ntoCvUZvbgSbNsH558Nhh8GMGfDrX4cbm5dfXv1Re8XFoUVLq1bhvbg4vX0NHQoPPRRurv74x2HUxareeAO++90wguHUqfC1r9X3yHZo1y7cGB07NrwfdVQYSjbRHXfA+++HURT32KPh+xSRRlLTFSBbr+ZWol+yJFSlQOjZ+NVXNadtzF6sv/lNWP9//qdyVcqbb7p36BB6Qy5alP52U/G3v4VfC127ur/9dpg3b1644XvqqZnZp4jUDlXdZMa774ZWNO3auT/xRN3pG3tcmquvDutffXWYfvvtMO7IgQe6l5bWb5upmj07jAuy667uxcVhRL8OHTK/XxFJrrZAr0cJ1tMTT4TOPJ06wb/+ldrDI2rq8FTfjlBjx8KKFXDLLaF36SOPhMfTTZsWHpqRSX37wttvhycIVTzk5J57Mr9fEUmfAn2atm0LHXpuuil07X/66dTrwHNzk3eEys2tX17MQn34ypXhkXJ5eSHI779//baXrs6dQ+erK68MHZwuvHDn7FdE0qNAn4Z160Ip/sknw2PrHngAdt019fXHjq38gG4IN2vHjq1/nnJyQkl+4MDQ8zUvr/7bqo82beCuu3buPkUkPWp1k6IFC+CII0IJ/s474eGH0wvyEKo4xo0LwdgsvI8bV/35rulq0wZ+9rOdH+RFpHlQiT4F//pXKC1v2gT/+Ed4wHB9jRjR8MAuIpIOlehr4R6GDD766PAE97feagbE7qYAAA7BSURBVFiQFxHJBgX6GixcGIb0veCC0Dnorbfg4IOznSsRkfQp0FexdWtoJtirF0yZArffHsZx79ix7nVFRJoi1dEneP99+MlPQun9hBPCAzN69Mh2rkREGkYlemDDBrjmGujfHz79NIw/M3mygryIxEOLD/RTp0JBQehd+qMfhcf6nXlmes/qbOhAZSIimdRiA/3y5XDeeXDMMaF1zZQp8Kc/hSEE0lFcHDpBLVgQtrNgQZhWsBeRpqJFBvrp06Fnz9Cj9OqrQ938McfUb1ujR1fu6QphevToBmdTRKRRtLibsVu3hjFZ9tgjlOILChq2vcYeqExEpLG1uED/9NMwdy48/njDgzw0/kBlIiKNrUVV3bjDzTfDN74BP/hB42xz7NjqT5Fq6EBlIiKNqUUF+uefh/feg6uuCqM+NoZMDVQmItJYLDyYpOkoKirykpKSjGz7yCNDNcu8eWHERxGRuDCzd9y9KNmylEr0ZjbEzOaa2TwzuyrJ8lwzm2Zm75nZbDM7MZqfb2YbzGxm9HqgYYdSf6++Cq+9Fh6SoSAvIi1JnTdjzSwHuBc4DigFZpjZJHefk5DsWmCCu99vZr2A54D8aNl/3P2Qxs12+saOhX33hfPPz3ZORER2rlRK9AOAee7+qbtvBsYDJ1dJ48Ae0ec9gcWNl8WGe+cdeOEF+PnPoV27bOdGRGTnSiXQdwMWJkyXRvMSjQF+ZGalhNL8qIRlPaIqnX+a2ZHJdmBmI82sxMxKysrKUs99im65BfbcE/7f/0tvPQ1tICJxkEqgTzbqS9U7uMOBP7t7d+BE4BEzawUsAXLdvRC4DHjMzPaosi7uPs7di9y9qHPnzukdQR0+/BCeegpGjQqdpFKloQ1EJC5SCfSlwP4J092pXjVzPjABwN3fANoCndx9k7svj+a/A/wH+EZDM52OW28N1TWXXpreehraQETiIpVAPwM4yMx6mFkbYBgwqUqaz4FjAMysJyHQl5lZ5+hmLmZ2AHAQ8GljZb4u8+fvKJl36pTeuhraQETios5A7+7lwCXAC8CHhNY1H5jZTWZ2UpTscuAnZjYL+CtwjocG+oOA2dH8icCF7r4iEweSzG9+E+rXr7gi/XVrGsJAQxuISHMT2w5TS5aEB4ecdVboqZquil8CidU37dur16uINE0N7jDVHN15J2zZEjpI1YeGNhCRuIjl6JUrVoTnvQ4bBl//ev23M2KEAruINH+xLNHffTesWxcGLxMRaeliF+jXrIHf/Q5OOgn69s12bkREsi92gf7BB2HlSrjmmmznRESkaYhVoN+4Ee64Izz/9bDDsp0bEZGmIVY3Y//0J/jiCw1TICKSKDYl+i1b4LbbQkn+6KOznRsRkaYjNoG+tDR0aBo9OrR7FxGRIDZVNz16wPvvK8iLiFQVm0APYVwbERGpTKFRRCTmFOhFRGJOgV5EJOYU6EVEYk6BXkQk5hToRURiToFeRCTmFOhFRGJOgV5EJOYU6EVEYk6BXkQk5hToRURirsUF+uJiyM8PA6Dl5+shJSISfykFejMbYmZzzWyemV2VZHmumU0zs/fMbLaZnZiw7OpovblmdkJjZj5dxcUwciQsWADu4X3kSAV7EYm3OgO9meUA9wLfBXoBw82sV5Vk1wIT3L0QGAbcF63bK5ruDQwB7ou2lxWjR8P69ZXnrV8f5ouIxFUqJfoBwDx3/9TdNwPjgZOrpHFgj+jznsDi6PPJwHh33+Tu84F50fay4vPP05svIhIHqQT6bsDChOnSaF6iMcCPzKwUeA4Ylca6mNlIMysxs5KysrIUs56+3Nz05ouIxEEqgT7Zw/m8yvRw4M/u3h04EXjEzFqluC7uPs7di9y9qHPnzilkqX7Gjg3PlU3Uvn2YLyISV6kE+lJg/4Tp7uyomqlwPjABwN3fANoCnVJcd6cZMQLGjYO8vPBs2by8MD1iRLZyJCKSeakE+hnAQWbWw8zaEG6uTqqS5nPgGAAz60kI9GVRumFmtquZ9QAOAt5urMzXx4gR8NlnsG1beFeQF5G4q/Ph4O5ebmaXAC8AOcDD7v6Bmd0ElLj7JOBy4CEz+zmhauYcd3fgAzObAMwByoGL3X1rpg5GRESqsxCPm46ioiIvKSnJdjZERJoVM3vH3YuSLWtxPWNFRFoaBXoRkZhToBcRiTkFehGRmFOgFxGJOQV6EZGYU6AXEYk5BXoRkZhToBcRiTkFehGRmFOgFxGJOQV6EZGYU6AXEYk5BXoRkZhToBcRiTkFehGRmFOgFxGJOQV6EZGYU6AXEYk5BXoRkZhToBcRiTkFehGRmFOgFxGJOQV6EZGYU6AXEYm5lAK9mQ0xs7lmNs/Mrkqy/LdmNjN6fWxmqxKWbU1YNqkxMy8iInXbpa4EZpYD3AscB5QCM8xskrvPqUjj7j9PSD8KKEzYxAZ3P6TxsiwiIulIpUQ/AJjn7p+6+2ZgPHByLemHA39tjMyJiEjDpRLouwELE6ZLo3nVmFke0AOYmjC7rZmVmNmbZnZKDeuNjNKUlJWVpZh1ERFJRSqB3pLM8xrSDgMmuvvWhHm57l4EnAncZWYHVtuY+zh3L3L3os6dO6eQJRERSVUqgb4U2D9hujuwuIa0w6hSbePui6P3T4HpVK6/FxGRDEsl0M8ADjKzHmbWhhDMq7WeMbP/AjoCbyTM62hmu0afOwFHAHOqrisiIplTZ6sbdy83s0uAF4Ac4GF3/8DMbgJK3L0i6A8Hxrt7YrVOT+BBM9tGuKjcmthaR0REMs8qx+XsKyoq8pKSkmxnQ0SkWTGzd6L7odWoZ6yISMwp0IuIxJwCvYhIzCnQi4jEnAK9iEjMKdCLiMScAr2ISMwp0IuIxJwCvYhIzCnQi4jEnAK9iEjMKdCLiMScAr2ISMwp0IuIxJwCvYhIzCnQi4jEnAK9iEjMKdCLiMScAr2ISMwp0IuIxJwCvYhIzCnQi4jEnAK9iEjMKdCLiMRcSoHezIaY2Vwzm2dmVyVZ/lszmxm9PjazVQnLzjazT6LX2Y2ZeRERqdsudSUwsxzgXuA4oBSYYWaT3H1ORRp3/3lC+lFAYfR5b+AGoAhw4J1o3ZWNehQiIlKjVEr0A4B57v6pu28GxgMn15J+OPDX6PMJwEvuviIK7i8BQxqSYRERSU8qgb4bsDBhujSaV42Z5QE9gKnprGtmI82sxMxKysrKUsm3iIikKJVAb0nmeQ1phwET3X1rOuu6+zh3L3L3os6dO6eQJRERSVUqgb4U2D9hujuwuIa0w9hRbZPuuiIikgGpBPoZwEFm1sPM2hCC+aSqiczsv4COwBsJs18AjjezjmbWETg+miciIjtJna1u3L3czC4hBOgc4GF3/8DMbgJK3L0i6A8Hxru7J6y7wsx+RbhYANzk7isa9xBERKQ2lhCXm4SioiIvKSnJdjZERJoVM3vH3YuSLVPPWBGRmFOgFxGJOQV6EZGYU6AXEYk5BXoRkZhToBcRiTkFehGRmItNoC8uhvx8aNUqvBcXZztHIiJNQ509Y5uD4mIYORLWrw/TCxaEaYARI7KXLxGRpiAWJfrRo3cE+Qrr14f5IiItXSwC/eefpzdfRKQliUWgz81Nb76ISEsSi0A/diy0b195Xvv2Yb6ISEsXi0A/YgSMGwd5eWAW3seN041YERGISasbCEFdgV1EpLpYlOhFRKRmCvQiIjGnQC8iEnMK9CIiMadALyISc03u4eBmVgYsqDK7E/BlFrKTSXE7prgdD8TvmOJ2PBC/Y2rI8eS5e+dkC5pcoE/GzEpqerp5cxW3Y4rb8UD8jiluxwPxO6ZMHY+qbkREYk6BXkQk5ppLoB+X7QxkQNyOKW7HA/E7prgdD8TvmDJyPM2ijl5EROqvuZToRUSknhToRURirskHejMbYmZzzWyemV2V7fw0lJl9Zmbvm9lMMyvJdn7qw8weNrNlZvbvhHl7m9lLZvZJ9N4xm3lMRw3HM8bMFkXnaaaZnZjNPKbLzPY3s2lm9qGZfWBml0bzm+V5quV4mu15MrO2Zva2mc2KjunGaH4PM3srOkePm1mbBu+rKdfRm1kO8DFwHFAKzACGu/ucrGasAczsM6DI3ZttJw8zGwSsBf7P3ftE824DVrj7rdEFuaO7/zKb+UxVDcczBljr7rdnM2/1ZWZdgC7u/q6ZdQDeAU4BzqEZnqdajueHNNPzZGYG7Obua82sNfAacClwGfCUu483sweAWe5+f0P21dRL9AOAee7+qbtvBsYDJ2c5Ty2eu78CrKgy+2TgL9HnvxD+CZuFGo6nWXP3Je7+bvR5DfAh0I1mep5qOZ5my4O10WTr6OXAd4CJ0fxGOUdNPdB3AxYmTJfSzE8u4US+aGbvmNnIbGemEX3N3ZdA+KcE9s1yfhrDJWY2O6raaRZVHMmYWT5QCLxFDM5TleOBZnyezCzHzGYCy4CXgP8Aq9y9PErSKDGvqQd6SzKv6dY1peYId+8PfBe4OKo2kKbnfuBA4BBgCXBHdrNTP2a2O/Ak8DN3/yrb+WmoJMfTrM+Tu29190OA7oQajJ7JkjV0P0090JcC+ydMdwcWZykvjcLdF0fvy4CnCSc3DpZG9agV9anLspyfBnH3pdE/4TbgIZrheYrqfZ8Eit39qWh2sz1PyY4nDucJwN1XAdOBw4G9zKziMa+NEvOaeqCfARwU3YVuAwwDJmU5T/VmZrtFN5Iws92A44F/175WszEJODv6fDbwTBbz0mAVwTByKs3sPEU3+v4IfOjudyYsapbnqabjac7nycw6m9le0ed2wLGEew/TgNOiZI1yjpp0qxuAqLnUXUAO8LC7j81ylurNzA4glOIhPJj9seZ4PGb2V2AwYUjVpcANwN+ACUAu8Dlwurs3ixucNRzPYEJ1gAOfAf9TUbfdHJjZt4FXgfeBbdHsawj12s3uPNVyPMNppufJzAoIN1tzCIXuCe5+UxQnxgN7A+8BP3L3TQ3aV1MP9CIi0jBNvepGREQaSIFeRCTmFOhFRGJOgV5EJOYU6EVEYk6BXkQk5hToRURi7v8DQRAaaEyt9+AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history[\"acc\"]\n",
    "val_acc = history.history[\"val_acc\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_los = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, \"bo\", label=\"training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"validation acc\")\n",
    "plt.title(\"training and validation acc\")\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Fine-tune a pre-trained convnet for feature extration with a pre-trained classifier on top of it (use of image generators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
